lamorel_args:
  log_level: info
  allow_subgraph_use_whith_gradient: true
  distributed_setup_args:
    n_rl_processes: 1
    n_llm_processes: 1
  accelerate_args:
    config_file: accelerate/default_config.yaml
    machine_rank: 0
    num_machines: 2
    num_processes: 2
  llm_args:
    model_type: seq2seq
    model_path: t5-small
    pretrained: true
    model_parallelism_size: 1
    minibatch_size: 4
    synchronize_gpus_after_scoring: false
    empty_cuda_cache_after_scoring: false
  updater_args:
rl_script_args:
  path: ???
  seed: 0
  number_envs: 2
  num_steps: 100
  max_episode_steps: 3
  simplification_str: easy
  frames_per_proc: 40
  reward_shaping_beta: 0
  discount: 0.99
  lr: 1e-6
  beta1: 0.9
  beta2: 0.999
  gae_lambda: 0.99
  entropy_coef: 0.01
  value_loss_coef: 0.5
  max_grad_norm: 0.5
  adam_eps: 1e-5
  clip_eps: 0.2
  epochs: 4
  batch_size: 16
  size_action_space: 3
  saving_path_logs: '/home/tcarta/DLP/storage/logs'
  name_experiment: 'llm_gtl'
  name_model: 'T5small'
  saving_path_model: '/home/tcarta/DLP/storage/models'
  name_environment: 'BabyAI-GoTo-v0'
  number_episodes: 10
  zero_shot: False
  language: 'english'
  load_embedding: false
  use_action_heads: false
